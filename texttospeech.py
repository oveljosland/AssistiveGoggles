from multiprocessing import Process

import pyrealsense2
import numpy as np #gjør bilde til narray
import cv2 #bildbehandling + vise bilder
from google.cloud import vision #google vision api
import pyttsx3 #snakking

import realsense_camera


class TextToSpeech(Process):
    def __init__(self, camera: realsense_camera.PyRealSenseCamera, rate: int = 150, volume: float = 0.9):
        # Initialize the pyttsx3 engine
        super().__init__()
        self.camera: realsense_camera.PyRealSenseCamera = camera

        self.engine = pyttsx3.init()

        # Set the rate of speech
        self.engine.setProperty('rate', rate)

        # Set the volume
        self.engine.setProperty('volume', volume)

        self.texts = []

    def run(self):
        while True:
            # listen for key press
            key = cv2.waitKey(0)
            if key == ord('r'):
                self.get_text(self.camera.get_color_image())
                self.speak()

    def get_text(self, frame: pyrealsense2.video_frame):
        """Gets the text from the frame using the Google Vision API."""
        client = vision.ImageAnnotatorClient()

        # Convert the frame to a byte array
        _, encoded_image = cv2.imencode('.jpg', np.asanyarray(frame.get_data()))
        content = encoded_image.tobytes()

        # Create an Image object
        image = vision.Image(content=content)

        # Perform text detection
        response = client.text_detection(image=image)

        # Check for errors in the response
        if response.error.message:
            raise Exception(
                "{}\nFor more info on error messages, check: "
                "https://cloud.google.com/apis/design/errors".format(response.error.message)
            )

        self.texts = response.text_annotations[1:]  # Skipping the first result as it's the full text

    def speak(self, texts = None):
        """Reads the texts out loud, defaulting to the stored texts if none are provided."""
        print("Texts:")
        texts = texts if texts else self.texts

        # If the input is not a list of texts
        if isinstance(texts, str):
            print(f'\t"{texts}"')
            self.engine.say(texts)
            self.engine.runAndWait()
            return

        # read out loud
        for text in texts:
            if isinstance(text, vision.TextAnnotation):
                print(f'\t"{text.description}"')
            else:
                print(f'\t"{text}"')

            self.engine.say(text)
            self.engine.runAndWait()


def run(camera: realsense_camera.PyRealSenseCamera):
    tts = TextToSpeech(camera)
    tts.start()


def main():
    camera = realsense_camera.PyRealSenseCamera()
    run(camera)


if __name__ == "__main__":
    main()

"""
import pyrealsense2 as rs #kameramodul
import numpy as np #gjør bilde til narray
import cv2 #bildbehandling + vise bilder
from google.cloud import vision #google vision api
import pyttsx3 #snakking

# Initialize the pyttsx3 engine
engine = pyttsx3.init()

# Set the rate of speech
engine.setProperty('rate', 150)

# Set the volume
engine.setProperty('volume', 0.9)

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()

## Get device product line for setting a supporting resolution
#pipeline_wrapper = rs.pipeline_wrapper(pipeline)
#pipeline_profile = config.resolve(pipeline_wrapper)
#device = pipeline_profile.get_device()
#device_product_line = str(device.get_info(rs.camera_info.product_line))
#
#found_rgb = False
#for s in device.sensors:
#    if s.get_info(rs.camera_info.name) == 'RGB Camera':
#        found_rgb = True
#        break
#if not found_rgb:
#    print("The demo requires Depth camera with Color sensor")
#    exit(0)

config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)

# Start streaming
pipeline.start(config)

def read(frame):
    client = vision.ImageAnnotatorClient()

    # Convert the frame to a byte array
    _, encoded_image = cv2.imencode('.jpg', np.asanyarray(frame.get_data()))
    content = encoded_image.tobytes()

    # Create an Image object
    image = vision.Image(content=content)

    # Perform text detection
    response = client.text_detection(image=image)
    texts = response.text_annotations
    print("Texts:")

    #read out loud
    for text in texts[1:]:  # Skipping the first result as it's the full text
        print(f'\n"{text.description}"')
        engine.say(text.description)
        engine.runAndWait()

    # Check for errors in the response
    if response.error.message:
        raise Exception(
            "{}\nFor more info on error messages, check: "
            "https://cloud.google.com/apis/design/errors".format(response.error.message)
        )


try:
    while True:

        # Wait for a coherent pair of frames: depth and color
        frames = pipeline.wait_for_frames()
        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        if not depth_frame or not color_frame:
            continue

        # Convert images to numpy arrays
        depth_image = np.asanyarray(depth_frame.get_data())
        color_image = np.asanyarray(color_frame.get_data())

        #listen for key press
        key = cv2.waitKey(1)
        if key == ord('r'):
            read(color_frame)

        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

        depth_colormap_dim = depth_colormap.shape
        color_colormap_dim = color_image.shape

        # If depth and color resolutions are different, resize color image to match depth image for display
        if depth_colormap_dim != color_colormap_dim:
            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)
            images = np.hstack((resized_color_image, depth_colormap))
        else:
            images = np.hstack((color_image, depth_colormap))

        # Show images
        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)
        cv2.imshow('RealSense', images)
        cv2.waitKey(1)

finally:

    # Stop streaming
    pipeline.stop()
"""